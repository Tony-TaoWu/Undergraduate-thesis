---
title: "INLA-example"
author: "Wu Wentao"
date: "2020/2/21"
output: html_document
---


```{r message=FALSE,error=FALSE}
library(INLA)
data(Seeds)
?Seeds
head(Seeds)
# - r is the number of seed germinated (successes)
# - n is the number of seeds attempted (trials)
# - x1 is the type of seed
# - x2 is the type of root extract
# - plate is the numbering of the plates/experiments

# All the covariates are 0/1 factors, and the numbering of the plates are arbitrary. We do not re-scale any covariates. The observations are integers, so we do not re-scale these either.

df = data.frame(y = Seeds$r, Ntrials = Seeds$n, Seeds[, 3:5])
# Explore data
summary(df)
table(df$x1)
table(df$x2)
plot(df)

##########################
##### Likehood
##########################
family1 = "binomial"
control.family1 = list(control.link=list(model="logit"))
# number of trials is df$Ntrials

# This specifies which likelihood we are going to use for the observations. The binomial distribution is defined with a certain number of trials, in INLA known as Ntrials. If there were hyper-parameters in our likelihood, we would specify the priors on these in control.family
# 这指定了我们将使用哪种可能性进行观测。二项分布由一定数量的试验定义，在INLA中称为Ntrials。如果在我们的可能性中有超参数，我们会在control.family中指定这些先验
```

#### Mathematical description

The precise mathematical description of our likelihood is, \[y_i \sim \text{Binom}(N_i, p_i) \] with \[\eta_i = \text{logit}(p_i). \] We call \(\eta_i\) (“eta i”) the predictor (or: linear predictor, latent variable, hidden state, …). This \(\eta_i\) is a parameter where we believe a linear model to be somewhat sensible. (A linear model is completely senseless on \(p_i\).)

### Formula

```{r}
hyper1 = list(theta = list(prior="pc.prec", param=c(1,0.01)))
formula1 = y ~ x1 + x2 + f(plate, model="iid", hyper=hyper1)
```

This specifies the formula, and the priors for any hyper-parameters in the random effects. See `inla.doc("pc.prec")` for more details on this prior

#### Mathematical description

The precise mathematical description of the model for our predictor \(\eta_i\) is \[\eta_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + v_i, \] where \[\beta_i \sim \mathcal N(0, 1000), \] \[v_i \sim \mathcal N(0, \sigma_v^2). \]

This is not yet a Bayesian model, since we have not defined the prior distribution (“simulating distribution”) for all parameters. We assume an exponential prior on \(v_i\), i.e. \[\pi(\sigma_v) = \lambda e^{-\lambda \sigma_v}. \] This is not yet a Bayesian model, as we have not defined a prior for \(\lambda\). We fix \(\lambda\) so that \[\pi(\sigma_v > 1) = 0.01, \] which means that \(\lambda = \frac{-log(0.01)}{1} \approx 4.6\).

Now we have a fully specified Bayesian model. After this, everything else is “just computations”, and then interpreting the results.

### Call INLA

Next we run the inla-call, where we collect the variables we have defined.

```{r}
res1 = inla(formula=formula1, data=df, 
            family=family1, Ntrials=Ntrials, 
            control.family=control.family1)
```

The `Ntrials` picks up the correct column in the dataframe.

### Look at results

```{r}
summary(res1)
```

This summary shows many elements of the results, notably not including the random effects (the parameters are not shown). The total result, however, is a complex high-dimensional posterior.

```{r}
# Run: str(res1, 1)
str(res1, 1)
```

This command, if run, shows one step in the list-of-lists hierarchy that is the inla result
如果运行此命令，将显示列表层次结构中的一个步骤，即inla结果

### Look at the random effect

```{r}
res1$summary.random$plate
```

### Plot the random effect quantiles

```{r}
plot(1:nrow(df), res1$summary.random$plate$`0.97`, col="red", ylim=c(-1,1),
     xlab="measurement number", ylab = "quantiles")
points(1:nrow(df), res1$summary.random$plate$`0.5quant`)
points(1:nrow(df), res1$summary.random$plate$`0.02`, col="blue")
```

The reason we use points instead of lines is that the numbering of the plates / experiments is arbitrary. Lines would falsely imply that one observation came “before/after” another (in time or on some other covariate).
我们使用点而不是线的原因是盘子/实验的编号是任意的。线条会错误地暗示一个观察结果出现在另一个观察结果之前/之后(在时间上或其他协变量上)。

### Plot the marginal of a fixed effect

```{r}
m.beta1 = inla.tmarginal(fun = function(x) x, marginal = 
                           res1$marginals.fixed$x1)
# - this transformation is the identity (does nothing)
# - m.beta1 is the marginal for the coefficient in front of the x1 covariate
plot(m.beta1, type="l", xlab = expression(beta[1]), ylab = "Probability density")
```

### Plot the marginal of a hyper-parameter

We must transform the marginal (`res1$marginals.hyperpar$"Precision for plate"`) to a parametrisation that makes sense to use for interpretation. The only parametrisation I like is \(\sigma\), marginal standard deviation. For numerical reasons, we need to transform the internal marginals. To find define the function used in the transformation, we look up the internal parametrisation, which is \(\log(precision)\), see `inla.doc("iid")`.

```{r}
m.sigma = inla.tmarginal(fun = function(x) exp(-1/2*x), marginal = 
                           res1$internal.marginals.hyperpar$`Log precision for plate`)
# - m.sigma is the marginal for the standard deviation parameter in the iid random effect
plot(m.sigma, type="l", xlab = expression(sigma[iid]), ylab = "Probability density")
```

### Plot the marginal of a parameter

```{r}
m.plate.7 = inla.tmarginal(fun = function(x) x, marginal = 
                           res1$marginals.random$plate$index.7)

# - m.plate.7 is one of the marginals for the parameters beloning to the plate iid effect
# - it is number 7, which corresponds to plate=7, which is our 7th row of data
plot(m.plate.7, type="l", xlab = "marginal plate nr 7", ylab = "Probability density")
```

### Plot the distribution of random effect estimates

Here we take the point estimates of the iid effect (posterior marginal medians).

```{r}
plot(density(res1$summary.random$plate$mean))
lines(0+c(-2, 2)*res1$summary.hyperpar$`0.5quant`^(-0.5) , c(0,0), col="blue")
# - draw a blue line for plus/minus 2 sigma
```

From this plot we see that the estimates of the random effect give a nice distribution, ending up within two standard deviations from zero. The assumption that the random effect is iid Gaussian looks reasonable

### Predictions

What if we want to predict? On the same plate? On a new plate? With new covariate values? Here we provide one example.

```{r}
df2 = rbind(df, c(NA, 1, 0, 0, 22))
tail(df2)
```

```{r}
res.pred = inla(formula=formula1, data=df2, 
            family=family1, Ntrials=Ntrials, 
            control.predictor = list(compute=T, link = 1),
            # - to get the posterior of the predictor and fitted values
            control.family=control.family1)

res.pred$summary.fitted.values[22, ]
# - this is the inv.logit(eta_i), namely p_i
```

### Comparing two models

An alternative to using the logit link would be to use the probit link. Changing the likelihood in this way also changes the behaviour of the linear predictor.

```{r}
control.family2 = list(control.link=list(model="probit"))
res1 = inla(formula=formula1, data=df, 
            family=family1, Ntrials=Ntrials, 
            control.predictor = list(compute=T, link=1),
            # - to get the posterior of the predictor and fitted values
            control.family=control.family1)

res2 = inla(formula=formula1, data=df, 
            family=family1, Ntrials=Ntrials, 
            control.predictor = list(compute=T, link=1),
            # - to get the posterior of the predictor and fitted values
            control.family=control.family2)

a = data.frame(y=df$y, ps=df$y/df$Ntrials,
               r1eta = res1$summary.linear.predictor$mean,
           r2eta = res2$summary.linear.predictor$mean,
           r1fit = res1$summary.fitted.values$mean,
           r2fit = res2$summary.fitted.values$mean)
round(a, 2)
```


### Comments
#### See also

Read the review paper (see Rue et al. 2016), and see the references therein! Also, explore www.r-inla.org, both the resources and the discussion group. And, use the function `inla.doc` in `R` to find information related to a keyword (e.g. a likelihood).

#### More details on `df` in general

This dataframe contains only numerical values, and all factors have been expanded. All covariates have a mean near zero and a standard deviation near 1 (i.e. 0.1<sd<10). They are also transformed to the appropriate scale, often with a `log`.

#### Mean, median or mode?

What point estimate should you use? Mean, median or mode?

The mode, is usually the wrong answer. This is where the posterior is maximised, which would be similar to a penalised maximum likelihood. But, it is generally understood that the median and mean are better, both as they have nicer statistical properties, a more “valid” loss function, and experience shows that they are more stable and less error prone.

Choosing between the median and mean is more difficult. In most cases, the difference is very small, especially for “good” parametrisations. I personally prefer the median (0.5 quantile), for several reasons

  + You can transform (between parametrisations) “directly” (eg using ^(-0.5)as in the code),
  + Estimates are independent of the internal choice of parameters (in the computer code).
  + I prefer the interpretation of the absolute loss function to that of a quadratic loss function for hyper-parameters.


###  The simple list

+ Hyper-parameter: A parameter controlling other parameters. The parameters controlling the structure in a random effect, e.g. range and sigma in a spatial effect. The parameter(s) in the observation likelihood.

+ Parameter(s): In theory, any parameter in the model, i.e. any random variable except Y.In practice, used to denote the large set of Gaussian variables in a random effect, or in the

+ Family: Same as likelihood, see likelihood.

+ Observation likelihood: Same as likelihood, see likelihood.

+ Likelihood: The distributions for the observations given the model for the predictor (formula).

+ Predictor: Symbol $\eta$.The model, represented by the formula. This is assumed to be Gaussian when the hyper-parameters are fixed/known. This is where you believe the effects in the model to be additive, that they add up to the total underlying true distribution/intensity.

+ Linear predictor: Same as predictor, see predictor. There is no assumption that the predictor is a linear regression, it can be any additive model.

+ Latent variable: Same as predictor, see predictor.
















